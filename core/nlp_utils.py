import spacy
from typing import List, Dict
import re
import dateparser
from  .strongs_words import strongs_verbs
from collections import defaultdict, Counter

# Chargement des modÃ¨les au dÃ©marrage
MODELS = {
    "fr": spacy.load("fr_core_news_md"),
    "en": spacy.load("en_core_web_md"),
}

def get_nlp(language: str = "fr"):
    """Retourne le pipeline spaCy selon la langue choisie (fr ou en)."""
    return MODELS.get(language, MODELS["fr"])


def extract_named_entities(text: str, language: str = "fr") -> List[Dict]:
    """
    Extrait les entitÃ©s nommÃ©es (personnes, lieux, dates, organisations, etc.)
    Retourne une liste de dictionnaires avec : texte, type, position.
    """
    nlp = get_nlp(language)
    doc = nlp(text)

    entities = []
    for ent in doc.ents:
        entities.append({
            "text": ent.text,
            "label": ent.label_,
            "start_char": ent.start_char,
            "end_char": ent.end_char,
        })

    return entities


def extract_numbers_and_units(text: str) -> List[Dict]:
    """
    Extrait les nombres suivis de mots (unitÃ©s), en filtrant les faux positifs.
    Exemples : '43 ouvriers', '30 Ã©tages', '7.9 de magnitude'
    """
    # Regex : nombre avec , ou . + mot (Ã©ventuellement prÃ©cÃ©dÃ© de "de")
    pattern = r"\b(\d+(?:[.,]\d+)?)\s+(?:de\s+)?([a-zA-ZÃ©Ã¨ÃªÃ§Ã»Ã®Ã¢Ã ]+)"
    matches = re.finditer(pattern, text)

    # Mots Ã  exclure comme unitÃ©s (verbes, auxiliaires, etc.)
    EXCLUDED_UNITS = {"a", "est", "sont", "ont", "Ã©tÃ©", "sera", "seront", "avoir", "Ãªtre"}

    results = []
    for match in matches:
        unit = match.group(2).lower()
        if unit in EXCLUDED_UNITS:
            continue

        value = match.group(1).replace(",", ".")  # 7,9 â†’ 7.9
        try:
            value = float(value) if "." in value else int(value)
        except ValueError:
            continue

        results.append({
            "value": value,
            "unit": unit,
            "start": match.start(),
            "end": match.end(),
        })

    return results


def extract_dates(text: str, language: str = "fr") -> List[Dict]:
    """
    Extrait les dates Ã  partir du texte, sous divers formats.
    Utilise regex + dateparser pour fiabiliser la conversion.
    """
    # Expressions classiques : 2 fÃ©vrier, 03/04/2025, mars 2023, le 10 janvier
    pattern = r"\b(?:le\s+)?(\d{1,2}[/-]\d{1,2}[/-]\d{2,4}|\d{1,2}\s+[a-zÃ©Ã»]+|[a-zÃ©Ã»]+\s+\d{4})\b"
    matches = re.finditer(pattern, text, flags=re.IGNORECASE)

    results = []
    for match in matches:
        date_str = match.group(1)
        parsed = dateparser.parse(date_str, languages=[language])

        if parsed:
            results.append({
                "text": date_str,
                "parsed_date": parsed.isoformat(),
                "start": match.start(),
                "end": match.end(),
            })

    return results


def extract_strong_verbs(text: str, language: str = "fr") -> List[Dict]:
    """
    Extrait les verbes forts prÃ©sents dans le texte en les comparant
    avec la liste de strongs_verbs. Utilise la lemmatisation pour fiabilitÃ©.
    """
    nlp = get_nlp(language)
    doc = nlp(text)

    strong_hits = []
    for token in doc:
        # VÃ©rifie si le token est un verbe et si sa forme de base est dans strongs_verbs
        if token.pos_ == "VERB":
            lemma = token.lemma_.lower()
            if lemma in strongs_verbs:
                strong_hits.append({
                    "text": token.text,
                    "lemma": lemma,
                    "start": token.idx,
                    "end": token.idx + len(token.text)
                })

    return strong_hits

def format_entities(text: str, language: str = "fr") -> Dict:
    """
    Regroupe toutes les entitÃ©s extraites dans un dictionnaire lisible.
    """
    return {
        "named_entities": extract_named_entities(text, language),
        "numbers": extract_numbers_and_units(text),
        "dates": extract_dates(text, language),
        "strong_verbs": extract_strong_verbs(text, language),
    }

def compute_datafication_score(entities: Dict, text: str) -> Dict:
    """
    Calcule un score de "datafication" pour un texte donnÃ© en fonction des entitÃ©s dÃ©tectÃ©es
    et de la densitÃ© de contenu structurÃ©. Le score est modulÃ© par un facteur de crÃ©dibilitÃ©
    basÃ© sur la densitÃ© d'information.

    Args:
        entities (Dict): Un dictionnaire contenant des listes d'entitÃ©s dÃ©tectÃ©es, 
                         telles que "numbers", "dates", "named_entities", et "strong_verbs".
        text (str): Le texte Ã  analyser.

    Returns:
        Dict: Un dictionnaire contenant :
            - "score" (int): Le score de datafication (maximum 10).
            - "justifications" (List[str]): Les raisons expliquant le score attribuÃ©.
            - "density" (float): La densitÃ© de contenu structurÃ© par rapport au nombre de mots.
            - "structured_items" (int): Le nombre total d'Ã©lÃ©ments structurÃ©s dÃ©tectÃ©s.
            - "word_count" (int): Le nombre total de mots dans le texte.
    """
    score = 0
    justifications = []

    total_items = sum(len(v) for v in entities.values())

    if len(entities.get("numbers", [])) >= 2:
        score += 3
        justifications.append("Plusieurs donnÃ©es chiffrÃ©es dÃ©tectÃ©es")

    if len(entities.get("dates", [])) >= 1:
        score += 2
        justifications.append("Date(s) clairement identifiÃ©e(s)")

    if len(entities.get("named_entities", [])) >= 2:
        score += 2
        justifications.append("EntitÃ©s nommÃ©es multiples (lieux, personnes, org)")

    if len(entities.get("strong_verbs", [])) >= 1:
        score += 2
        justifications.append("Verbe(s) fort(s) signalant un impact ou une action")

    # PondÃ©ration selon la densitÃ© de contenu structurÃ©
    words = len(text.split())
    density = total_items / words if words else 0

    if density < 0.01:
        score = max(score - 2, 0)
        justifications.append("Faible densitÃ© d'information datafiable pour la longueur de l'article")

    return {
        "score": min(score, 10),
        "justifications": justifications,
        "density": round(density, 3),
        "structured_items": total_items,
        "word_count": words
    }

def group_named_entities(entities: list[dict]) -> dict:
    grouped = defaultdict(Counter)
    for ent in entities:
        label = ent["label"]
        text = ent["text"]
        grouped[label][text] += 1
    return grouped


def interpret_datafication_score(score: int) -> str:
    if score >= 9:
        return "TrÃ¨s bon potentiel de datajournalisme (chiffrÃ©, localisÃ©, visualisable)"
    elif score >= 7:
        return "Bon potentiel de datafication avec Ã©lÃ©ments structurÃ©s exploitables"
    elif score >= 5:
        return "Potentiel modÃ©rÃ© : quelques Ã©lÃ©ments chiffrÃ©s ou datÃ©s identifiables"
    elif score >= 3:
        return "Faible potentiel data : surtout narratif, peu structurÃ©"
    else:
        return "TrÃ¨s faible ou absent : article descriptif sans donnÃ©es exploitables"


def get_article_profile(entities: dict, score_data: dict) -> str:
    """
    GÃ©nÃ¨re un profil Ã©ditorial basÃ© sur les entitÃ©s, la densitÃ©, et le score de datafication.
    """
    n_verbs = len(entities.get("strong_verbs", []))
    n_numbers = len(entities.get("numbers", []))
    n_dates = len(entities.get("dates", []))
    n_places = sum(1 for ent in entities.get("named_entities", []) if ent["label"] == "LOC")
    density = score_data.get("density", 0)
    score = score_data.get("score", 0)

    if score >= 9 and density > 0.15 and n_verbs >= 2:
        return "ğŸ“Š Datajournalisme potentiel Ã©levÃ© â€“ structurÃ©, chiffrÃ© et dynamique"
    elif n_numbers >= 3 and n_dates >= 2 and n_places >= 2:
        return "ğŸ“ LocalisÃ© et temporel â€“ structurÃ© autour de donnÃ©es concrÃ¨tes"
    elif n_numbers <= 1 and n_verbs == 0 and density < 0.05:
        return "ğŸ“£ Narratif ou descriptif â€“ peu de donnÃ©es exploitables"
    elif score >= 6 and n_places >= 2 and n_verbs == 0:
        return "ğŸ§® StructurÃ© et quantifiable â€“ intÃ©ressant pour un angle local"
    else:
        return "ğŸ”¬ Exploratoire ou symbolique â€“ sujet riche mais peu structurÃ©"
